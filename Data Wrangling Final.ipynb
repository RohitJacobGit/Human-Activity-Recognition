{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy import stats\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from matplotlib.pyplot import figure\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the training file\n",
    "df_train_csv = pd.read_csv('files/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the individual subjects from training set\n",
    "df_train_csv_subjects = list(dict(df_train_csv['Subject'].value_counts()).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dfs = [] # To store the dataframes\n",
    "\n",
    "def parse_subject_files(subject,all_files):\n",
    "    '''\n",
    "    Input: Subject in the train file and all the instances(files) of that subject\n",
    "    \n",
    "    1. Loop through all the files(instances) of the subject present the subject folder\n",
    "    2. Validating the subject in the train file.\n",
    "    3. Add the list of dataframes into list_of_dfs\n",
    "    \n",
    "    '''\n",
    "    print(subject,' : started processing.....')\n",
    "    \n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename, header=None)\n",
    "        if filename.split(sep='/')[7] in list(df_train_csv['Datafile'].str\n",
    "                                                                      .partition('/')[2]):\n",
    "            current_label = (df_train_csv[df_train_csv['Datafile']==\n",
    "                                          (''.join([subject,'/',filename.split(sep='/')[7]]))]\n",
    "                                          ['Label'].values[0])\n",
    "            df['Label'] = current_label\n",
    "            list_of_dfs.append(df)\n",
    "            \n",
    "    print(subject,' : completed processing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each subject in the train file and call the parse_subject_files definition\n",
    "for i in range(len(df_train_csv_subjects)):\n",
    "    path = r'/Users/rohitanand/Documents/bbdc2019/files/'+df_train_csv_subjects[i]\n",
    "    all_files = glob.glob(path + \"/*.csv\")\n",
    "    parse_subject_files(df_train_csv_subjects[i],all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat all the files in a single frame\n",
    "df_train_main = pd.concat(list_of_dfs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "# A 'ground truth' column named as Label has been added as our y-variable\n",
    "df_train_main.columns = ['EMG1',\n",
    "                'EMG2',\n",
    "                'EMG3',\n",
    "                'EMG4',\n",
    "                'Airborne',\n",
    "                'ACC upper X',\n",
    "                'ACC upper Y',\n",
    "                'ACC upper Z',\n",
    "                'Goniometer X',\n",
    "                'ACC lower X',\n",
    "                'ACC lower Y',\n",
    "                'ACC loewr Z',\n",
    "                'Goniometer Y',\n",
    "                'Gyro upper X',\n",
    "                'Gyro upper Y',\n",
    "                'Gyro upper Z',\n",
    "                'Gyro lower X',\n",
    "                'Gyro lower Y',\n",
    "                'Gyro lower Z',\n",
    "                'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df_train_main['Label'] = label_encoder.fit_transform(df_train_main['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_scores = stats.zscore(df_train_main)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 1).all(axis=1)\n",
    "df_train_main = df_train_main[filtered_entries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_train_main.iloc[:,:-1].values\n",
    "y = df_train_main.iloc[:,19].values\n",
    "\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "onehotencoder = OneHotEncoder(categories='auto')\n",
    "y = onehotencoder.fit_transform(y).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train-test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20)\n",
    "\n",
    "# Feature Scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 8) #found this to be the optimal number of columns \n",
    "x_train = pca.fit_transform(x_train)\n",
    "x_test = pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the original dataframe\n",
    "figure(num=None, figsize=(20, 18), dpi=80, facecolor='w', edgecolor='k')\n",
    "sns.heatmap(df_train_main.corr(), annot=True, fmt=\".2%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after PCA we found 8 columns which can be selected \n",
    "figure(num=None, figsize=(8, 5), dpi=80, facecolor='w', edgecolor='k')\n",
    "sns.heatmap(pd.DataFrame(x_train).corr(), annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
